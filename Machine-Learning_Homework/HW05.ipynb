{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04f4f6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pdb\n",
    "import pprint\n",
    "import logging\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b53a7703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "import tqdm.auto as tqdm\n",
    "from pathlib import Path\n",
    "from argparse import Namespace\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57d31083",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 73\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "np.random.seed(seed)  \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcefda44",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "77f74358",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"F:/Ai/Data/ml2021spring-hw5/DATA/rawdata/ted2020\"\n",
    "prefix = Path(prefix).absolute()\n",
    "\n",
    "data_dir = \"F:/Ai/Data/ml2021spring-hw5/DATA/rawdata\"\n",
    "dataset_name = 'ted2020'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6586c62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_lang = 'en'\n",
    "tgt_lang = 'zh'\n",
    "\n",
    "data_prefix = f'{prefix}/train_dev.raw'\n",
    "test_prefix = f'{prefix}/test.raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cdb56922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\Ai\\\\Data\\\\ml2021spring-hw5\\\\DATA\\\\rawdata\\\\ted2020/train_dev.raw'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9dd119a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'head' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n",
      "'head' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!head {data_prefix+'.'+src_lang} -n 5\n",
    "!head {data_prefix+'.'+tgt_lang} -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "afecf830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\Ai\\\\Data\\\\ml2021spring-hw5\\\\DATA\\\\rawdata\\\\ted2020/train_dev.raw'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82a75956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a9412a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个函数，将字符串中的全角字符转换为半角字符\n",
    "def strQ2B(ustring):\n",
    "    # 创建一个空列表，用于存储转换后的字符串\n",
    "    ss = []\n",
    "    # 遍历输入的字符串\n",
    "    for s in ustring:\n",
    "        # 创建一个空字符串，用于存储转换后的字符\n",
    "        rstring = \"\"\n",
    "        # 遍历字符串中的每个字符\n",
    "        for uchar in s:\n",
    "            # 获取字符的Unicode编码\n",
    "            inside_code = ord(uchar)\n",
    "            # 如果字符的Unicode编码为全角空格，则将其转换为半角空格\n",
    "            if inside_code == 12288:\n",
    "                inside_code = 32\n",
    "            # 如果字符的Unicode编码在65281到65374之间，则将其转换为对应的半角字符\n",
    "            elif (inside_code >= 65281 and inside_code <= 65374):\n",
    "                inside_code -= 65248\n",
    "            # 将转换后的字符添加到rstring中\n",
    "            rstring += chr(inside_code)\n",
    "        # 将转换后的字符串添加到ss列表中\n",
    "        ss.append(rstring)\n",
    "    # 将ss列表中的字符串连接起来，并返回\n",
    "    return ''.join(ss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "275e7b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_s(s, lang):\n",
    "    # 如果语言是英文\n",
    "    if lang == 'en':\n",
    "        # 去掉括号及其内容\n",
    "        s = re.sub(r\"\\([^()]*\\)\", \"\", s)\n",
    "        # 去掉连字符\n",
    "        s = s.replace('-', '')\n",
    "        # 在标点符号前后加空格\n",
    "        s = re.sub('([.,;!?()\\\"])', r' \\1 ', s)\n",
    "    # 如果语言是中文\n",
    "    elif lang == 'zh':\n",
    "        # 将全角字符转换为半角字符\n",
    "        s = strQ2B(s)\n",
    "        # 去掉括号及其内容\n",
    "        s = re.sub(r\"\\([^()]*\\)\", \"\", s)\n",
    "        # 去掉空格\n",
    "        s = s.replace(' ', '')\n",
    "        # 去掉破折号\n",
    "        s = s.replace('—', '')\n",
    "        # 将中文引号替换为英文引号\n",
    "        s = s.replace('“', '\"')\n",
    "        s = s.replace('”', '\"')\n",
    "        # 去掉下划线\n",
    "        s = s.replace('_', '')\n",
    "        # 在标点符号前后加空格\n",
    "        s = re.sub('([。,;!?()\\\"~「」])', r' \\1 ', s)\n",
    "    # 去掉多余的空格\n",
    "    s = ' '.join(s.strip().split())\n",
    "    # 返回处理后的字符串\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2edaecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_s(s, lang):\n",
    "    # 判断语言类型，如果是中文，则返回字符串长度，否则返回字符串中单词的个数\n",
    "    if lang == 'zh':\n",
    "        return len(s)\n",
    "    return len(s.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ca7b8e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus(prefix, l1, l2, ratio=9, max_len=1000, min_len=1):\n",
    "    # 检查前缀为prefix的l1和l2文件是否存在，如果存在则跳过清理过程\n",
    "    if Path(f'{prefix}.clean.{l1}').exists() and Path(f'{prefix}.clean.{l2}').exists():\n",
    "        print(f'{prefix}.clean.{l1} & {l2} exists. skipping clean.')\n",
    "        return\n",
    "    # 打开前缀为prefix的l1和l2文件，以及清理后的l1和l2文件\n",
    "    with open(f'{prefix}.{l1}', 'r') as l1_in_f:\n",
    "        with open(f'{prefix}.{l2}', 'r') as l2_in_f:\n",
    "            with open(f'{prefix}.clean.{l1}', 'w') as l1_out_f:\n",
    "                with open(f'{prefix}.clean.{l2}', 'w') as l2_out_f:\n",
    "                    # 遍历l1文件中的每一行\n",
    "                    for s1 in l1_in_f:\n",
    "                        # 去除行首行尾的空格\n",
    "                        s1 = s1.strip()\n",
    "                        # 读取l2文件中的对应行\n",
    "                        s2 = l2_in_f.readline().strip()\n",
    "                        # 清理l1和l2文件中的内容\n",
    "                        s1 = clean_s(s1, l1)\n",
    "                        s2 = clean_s(s2, l2)\n",
    "                        # 计算l1和l2文件中的内容长度\n",
    "                        s1_len = len_s(s1, l1)\n",
    "                        s2_len = len_s(s2, l2)\n",
    "                        # 如果最小长度大于0，则去除长度小于最小长度的句子\n",
    "                        if min_len > 0: # remove short sentence\n",
    "                            if s1_len < min_len or s2_len < min_len:\n",
    "                                continue\n",
    "                        # 如果最大长度大于0，则去除长度大于最大长度的句子\n",
    "                        if max_len > 0: # remove long sentence\n",
    "                            if s1_len > max_len or s2_len > max_len:\n",
    "                                continue\n",
    "                        # 如果比例大于0，则去除长度比例大于比例的句子\n",
    "                        if ratio > 0: # remove by ratio of length\n",
    "                            if s1_len/s2_len > ratio or s2_len/s1_len > ratio:\n",
    "                                continue\n",
    "                        # 将清理后的l1和l2文件写入对应文件\n",
    "                        print(s1, file=l1_out_f)\n",
    "                        print(s2, file=l2_out_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "28096151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\Ai\\Data\\ml2021spring-hw5\\DATA\\rawdata\\ted2020/train_dev.raw.clean.en & zh exists. skipping clean.\n",
      "F:\\Ai\\Data\\ml2021spring-hw5\\DATA\\rawdata\\ted2020/test.raw.clean.en & zh exists. skipping clean.\n"
     ]
    }
   ],
   "source": [
    "clean_corpus(data_prefix, src_lang, tgt_lang)\n",
    "clean_corpus(test_prefix, src_lang, tgt_lang, ratio=-1, min_len=-1, max_len=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "03b51a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'head' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n",
      "'head' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!head {data_prefix+'.clean.'+src_lang} -n 5\n",
    "!head {data_prefix+'.clean.'+tgt_lang} -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5890c91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ratio = 0.01\n",
    "train_ratio = 1 - valid_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b3b4672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/valid splits exists. skipping split.\n"
     ]
    }
   ],
   "source": [
    "# 判断train.clean.src_lang和train.clean.tgt_lang以及valid.clean.src_lang和valid.clean.tgt_lang是否存在\n",
    "if (prefix / f'train.clean.{src_lang}').exists() and (prefix / f'train.clean.{tgt_lang}').exists() and \\\n",
    "   (prefix / f'valid.clean.{src_lang}').exists() and (prefix / f'valid.clean.{tgt_lang}').exists():\n",
    "    # 如果存在，则打印提示信息，并跳过split\n",
    "    print(f'train/valid splits exists. skipping split.')\n",
    "else:\n",
    "    # 如果不存在，则计算data_prefix.clean.src_lang文件的行数\n",
    "    line_num = sum(1 for line in open(f'{data_prefix}.clean.{src_lang}'))\n",
    "    # 生成一个与行数相同的标签列表\n",
    "    labels = list(range(line_num))\n",
    "    # 随机打乱标签列表\n",
    "    random.shuffle(labels)\n",
    "    # 遍历src_lang和tgt_lang\n",
    "    for lang in [src_lang, tgt_lang]:\n",
    "        # 打开train.clean.lang文件\n",
    "        train_f = open(os.path.join(data_dir, dataset_name, f'train.clean.{lang}'), 'w')\n",
    "        # 打开valid.clean.lang文件\n",
    "        valid_f = open(os.path.join(data_dir, dataset_name, f'valid.clean.{lang}'), 'w')\n",
    "        # 初始化计数器\n",
    "        count = 0\n",
    "        # 遍历data_prefix.clean.lang文件\n",
    "        for line in open(f'{data_prefix}.clean.{lang}', 'r'):\n",
    "            # 如果标签列表中的第count个元素除以行数小于train_ratio，则将行写入train.clean.lang文件\n",
    "            if labels[count]/line_num < train_ratio:\n",
    "                train_f.write(line)\n",
    "            # 否则将行写入valid.clean.lang文件\n",
    "            else:\n",
    "                valid_f.write(line)\n",
    "            # 计数器加1\n",
    "            count += 1\n",
    "        # 关闭train.clean.lang文件\n",
    "        train_f.close()\n",
    "        # 关闭valid.clean.lang文件\n",
    "        valid_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0fd9f42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\Ai\\Data\\ml2021spring-hw5\\DATA\\rawdata\\ted2020/spm8000.model exists. skipping spm_train.\n"
     ]
    }
   ],
   "source": [
    "## Subword Unit\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "vocab_size = 8000\n",
    "\n",
    "if (prefix/f'spm{vocab_size}.model').exists():\n",
    "    print(f'{prefix}/spm{vocab_size}.model exists. skipping spm_train.')\n",
    "else:\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=','.join([f'{prefix}/train.clean.{src_lang}',\n",
    "                        f'{prefix}/valid.clean.{src_lang}',\n",
    "                        f'{prefix}/train.clean.{tgt_lang}',\n",
    "                        f'{prefix}/valid.clean.{tgt_lang}']),\n",
    "        model_prefix = prefix/f'spm{vocab_size}',\n",
    "        vocab_size = vocab_size,\n",
    "        character_coverage = 1,\n",
    "        model_type = 'unigram',\n",
    "        input_sentence_size=1e6,\n",
    "        shuffle_input_sentence=True,\n",
    "        normalization_rule_name = 'nmt_nfkc_cf'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd9e9b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\Ai\\Data\\ml2021spring-hw5\\DATA\\rawdata\\ted2020\\train.en exists. skipping spm_encode.\n",
      "F:\\Ai\\Data\\ml2021spring-hw5\\DATA\\rawdata\\ted2020\\train.zh exists. skipping spm_encode.\n",
      "F:\\Ai\\Data\\ml2021spring-hw5\\DATA\\rawdata\\ted2020\\valid.en exists. skipping spm_encode.\n",
      "F:\\Ai\\Data\\ml2021spring-hw5\\DATA\\rawdata\\ted2020\\valid.zh exists. skipping spm_encode.\n",
      "F:\\Ai\\Data\\ml2021spring-hw5\\DATA\\rawdata\\ted2020\\test.en exists. skipping spm_encode.\n",
      "F:\\Ai\\Data\\ml2021spring-hw5\\DATA\\rawdata\\ted2020\\test.zh exists. skipping spm_encode.\n"
     ]
    }
   ],
   "source": [
    "spm_model = spm.SentencePieceProcessor(model_file=str(prefix/f'spm{vocab_size}.model'))\n",
    "in_tag = {\n",
    "    'train':'train.clean',\n",
    "    'valid':'valid.clean',\n",
    "    'test':'test.raw.clean',\n",
    "}\n",
    "\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    for lang in [src_lang, tgt_lang]:\n",
    "        out_path = prefix/f'{split}.{lang}'\n",
    "        if out_path.exists():\n",
    "            print(f\"{out_path} exists. skipping spm_encode.\")\n",
    "        else:\n",
    "            with open(prefix/f'{split}.{lang}', 'w') as out_f:\n",
    "                with open(prefix/f'{in_tag[split]}.{lang}', 'r') as in_f:\n",
    "                    for line in in_f:\n",
    "                        line = line.strip()\n",
    "                        tok = spm_model.encode(line, out_type=str)\n",
    "                        print(' '.join(tok), file=out_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f76c80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'head' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n",
      "�����﷨����ȷ��\n"
     ]
    }
   ],
   "source": [
    "!head {data_dir+'/'+dataset_name+'/train.'+src_lang} -n 5\n",
    "!head {data_dir+'/'+dataset_name+'/train.'+tgt_lang} -n 5|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5db4c425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 替换 fairseq 功能：构建共享词表\n",
    "binpath = Path('./DATA/data-bin', dataset_name)\n",
    "binpath.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cef14536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# 从分词后的训练数据构建词表\n",
    "vocab = Counter()\n",
    "for lang in [src_lang, tgt_lang]:\n",
    "    train_file = prefix / f'train.{lang}'\n",
    "    if train_file.exists():\n",
    "        with open(train_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                vocab.update(line.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c11d4ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预处理完成，分词数据保存在 F:\\Ai\\Data\\ml2021spring-hw5\\DATA\\rawdata\\ted2020，词表和索引数据保存在 DATA\\data-bin\\ted2020\n"
     ]
    }
   ],
   "source": [
    "special_tokens = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "vocab = special_tokens + [word for word, count in vocab.most_common(vocab_size - len(special_tokens))]\n",
    "\n",
    "# 保存词表\n",
    "vocab_file = binpath / 'vocab.txt'\n",
    "with open(vocab_file, 'w', encoding='utf-8') as f:\n",
    "    for word in vocab:\n",
    "        f.write(word + '\\n')\n",
    "\n",
    "# 生成索引数据（替代 fairseq 的二进制格式）\n",
    "word_to_id = {word: idx for idx, word in enumerate(vocab)}\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    for lang in [src_lang, tgt_lang]:\n",
    "        input_file = prefix / f'{split}.{lang}'\n",
    "        output_file = binpath / f'{split}.ids.{lang}'\n",
    "        if input_file.exists():\n",
    "            with open(input_file, 'r', encoding='utf-8') as f_in, \\\n",
    "                    open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "                for line in f_in:\n",
    "                    tokens = line.strip().split()\n",
    "                    ids = [word_to_id.get(token, word_to_id['<unk>']) for token in tokens]\n",
    "                    f_out.write(' '.join(map(str, ids)) + '\\n')\n",
    "\n",
    "print(f\"预处理完成，分词数据保存在 {prefix}，词表和索引数据保存在 {binpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b952b9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用 GPU:\n",
      "  - GPU 数量: 1\n",
      "  - 当前 GPU: NVIDIA GeForce RTX 4070 Ti SUPER\n",
      "  - CUDA 版本: 12.4\n",
      "计算设备: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 检查 CUDA 可用性\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(\"使用 GPU:\")\n",
    "    print(f\"  - GPU 数量: {torch.cuda.device_count()}\")\n",
    "    print(f\"  - 当前 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  - CUDA 版本: {torch.version.cuda}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"使用 CPU\")\n",
    "\n",
    "# 示例：后续使用 device\n",
    "print(f\"计算设备: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "da9be7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Namespace(\n",
    "    datadir = \"./DATA/data-bin/ted2020\",\n",
    "    savedir = \"./checkpoints/rnn\",\n",
    "    source_lang = \"en\",\n",
    "    target_lang = \"zh\",\n",
    "    \n",
    "    # cpu threads when fetching & processing data.\n",
    "    num_workers=2,  \n",
    "    # batch size in terms of tokens. gradient accumulation increases the effective batchsize.\n",
    "    max_tokens=8192,\n",
    "    accum_steps=2,\n",
    "    \n",
    "    # the lr s calculated from Noam lr scheduler. you can tune the maximum lr by this factor.\n",
    "    lr_factor=2.,\n",
    "    lr_warmup=4000,\n",
    "    \n",
    "    # clipping gradient norm helps alleviate gradient exploding\n",
    "    clip_norm=1.0,\n",
    "    \n",
    "    # maximum epochs for training\n",
    "    max_epoch=30,\n",
    "    start_epoch=1,\n",
    "    \n",
    "    # beam size for beam search\n",
    "    beam=5, \n",
    "    # generate sequences of maximum length ax + b, where x is the source length\n",
    "    max_len_a=1.2, \n",
    "    max_len_b=10,\n",
    "    # when decoding, post process sentence by removing sentencepiece symbols.\n",
    "    post_process = \"sentencepiece\",\n",
    "    \n",
    "    # checkpoints\n",
    "    keep_last_epochs=5,\n",
    "    resume=None, # if resume from checkpoint name (under config.savedir)\n",
    "    \n",
    "    # logging\n",
    "    use_wandb=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "58cad1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    level=\"INFO\", # \"DEBUG\" \"WARNING\" \"ERROR\"\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "proj = \"hw5.seq2seq\"\n",
    "logger = logging.getLogger(proj)\n",
    "if config.use_wandb:\n",
    "    import wandb\n",
    "    wandb.init(project=proj, name=Path(config.savedir).stem, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dd6707e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "stream did not contain valid UTF-8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m找不到 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspm_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m，请先训练 sentencepiece 模型\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# 加载 tokenizer\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mPreTrainedTokenizerFast\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mspm_model_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<bos>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<eos>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<unk>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<pad>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# 加载数据集\u001b[39;00m\n\u001b[0;32m     32\u001b[0m data_files \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m     34\u001b[0m         config\u001b[38;5;241m.\u001b[39msource_lang: \u001b[38;5;28mstr\u001b[39m(Path(config\u001b[38;5;241m.\u001b[39mdatadir) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39msource_lang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     },\n\u001b[0;32m     45\u001b[0m }\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:117\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(tokenizer_object)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fast_tokenizer_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m from_slow:\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# We have a serialization from tokenizers which let us directly build the backend\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mTokenizerFast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfast_tokenizer_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m slow_tokenizer:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;66;03m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[0;32m    120\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n",
      "\u001b[1;31mException\u001b[0m: stream did not contain valid UTF-8"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from torch.utils.data import DataLoader\n",
    "from argparse import Namespace\n",
    "\n",
    "# 配置\n",
    "config = Namespace(\n",
    "    datadir=\"./DATA/data-bin/ted2020\",\n",
    "    source_lang=\"en\",\n",
    "    target_lang=\"zh\",\n",
    "    num_workers=2,\n",
    "    max_tokens=8192,\n",
    ")\n",
    "\n",
    "# 确认 sentencepiece 模型\n",
    "spm_model_path = prefix / f'spm{vocab_size}.model'\n",
    "if not spm_model_path.exists():\n",
    "    raise FileNotFoundError(f\"找不到 {spm_model_path}，请先训练 sentencepiece 模型\")\n",
    "\n",
    "# 加载 tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=str(spm_model_path),\n",
    "    bos_token=\"<bos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    ")\n",
    "\n",
    "# 加载数据集\n",
    "data_files = {\n",
    "    \"train\": {\n",
    "        config.source_lang: str(Path(config.datadir) / f'train.{config.source_lang}'),\n",
    "        config.target_lang: str(Path(config.datadir) / f'train.{config.target_lang}'),\n",
    "    },\n",
    "    \"valid\": {\n",
    "        config.source_lang: str(Path(config.datadir) / f'valid.{config.source_lang}'),\n",
    "        config.target_lang: str(Path(config.datadir) / f'valid.{config.target_lang}'),\n",
    "    },\n",
    "    \"test\": {\n",
    "        config.source_lang: str(Path(config.datadir) / f'test.{config.source_lang}'),\n",
    "        config.target_lang: str(Path(config.datadir) / f'test.{config.target_lang}'),\n",
    "    },\n",
    "}\n",
    "\n",
    "dataset = load_dataset(\"text\", data_files=data_files)\n",
    "\n",
    "# 数据处理\n",
    "def process_data(examples):\n",
    "    src_texts = examples[config.source_lang]\n",
    "    tgt_texts = examples[config.target_lang]\n",
    "    encodings = tokenizer(\n",
    "        src_texts,\n",
    "        text_target=tgt_texts,\n",
    "        max_length=config.max_tokens // 2,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    for key in [\"input_ids\", \"labels\"]:\n",
    "        seq_len = encodings[key].shape[1]\n",
    "        padded_len = (seq_len + 7) // 8 * 8\n",
    "        if seq_len < padded_len:\n",
    "            encodings[key] = torch.nn.functional.pad(\n",
    "                encodings[key], (0, padded_len - seq_len), value=tokenizer.pad_token_id\n",
    "            )\n",
    "    encodings[\"attention_mask\"] = (encodings[\"input_ids\"] != tokenizer.pad_token_id).long()\n",
    "    return encodings\n",
    "\n",
    "# 处理数据集\n",
    "processed_dataset = dataset.map(\n",
    "    process_data,\n",
    "    batched=True,\n",
    "    remove_columns=[config.source_lang, config.target_lang],\n",
    ")\n",
    "\n",
    "# 创建 DataLoader\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"input_ids\": torch.stack([x[\"input_ids\"] for x in batch]),\n",
    "        \"attention_mask\": torch.stack([x[\"attention_mask\"] for x in batch]),\n",
    "        \"labels\": torch.stack([x[\"labels\"] for x in batch]),\n",
    "    }\n",
    "\n",
    "dataloaders = {\n",
    "    split: DataLoader(\n",
    "        processed_dataset[split],\n",
    "        batch_size=16,\n",
    "        shuffle=(split == \"train\"),\n",
    "        num_workers=config.num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    for split in [\"train\", \"valid\", \"test\"] if split in processed_dataset\n",
    "}\n",
    "\n",
    "print(f\"数据集加载完成：{dataloaders.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eab4135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('DATA/rawdata/ted2020/spm8000.model')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99fe977",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
