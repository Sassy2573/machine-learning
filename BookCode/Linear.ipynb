{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b93c85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "#生成一个样本量为100 特征量为10的数据集\n",
    "sample=100\n",
    "feature=10\n",
    "X=np.random.rand(sample,10)\n",
    "#构造目标值 y=3+2*x1+5*x2-3*x3\n",
    "y=3+2*X[:,0]+5*X[:,1]-3*X[:,2]+np.random.randn(sample) # 增加一点噪音\n",
    "#np.random.randn 生成满足正态分布的随机数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cf1fb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR():\n",
    "    def __init__(self):\n",
    "        self.w=None\n",
    "        self.b=None \n",
    "        \n",
    "    def get_loss(self,y,y_pre):\n",
    "        \"\"\"\n",
    "        定义损失函数f(x)--> 均方误差\n",
    "        y: 真实值\n",
    "        y_pre: 预测值\n",
    "        \"\"\"\n",
    "        loss=np.mean((y-y_pre)**2)\n",
    "        return loss\n",
    "    \n",
    "    def fit(self,x,y,learning_rate=0.01,n_iterations=500):\n",
    "        \"\"\"\n",
    "        x 训练数据\n",
    "        y 目标值\n",
    "        learning_rate 学习率\n",
    "        n_iterations 迭代次数\n",
    "        \"\"\"\n",
    "        sample,feature=x.shape\n",
    "        \n",
    "        if self.w==None:\n",
    "            #初始化数据\n",
    "            self.w=np.random.randn(feature)\n",
    "            self.b=0\n",
    "            \n",
    "        #开始训练\n",
    "        for i in range(n_iterations):\n",
    "            y_pre=np.dot(x,self.w)+self.b\n",
    "            #计算预测值与真实值之差  \n",
    "            # !!注意这里是预测值减真实值\n",
    "            diff=y_pre-y\n",
    "            #计算损失函数的梯度\n",
    "            dw=2/sample*np.dot(x.T,diff)\n",
    "            db=2/sample*np.sum(diff,axis=0)\n",
    "            \n",
    "            #更新参数\n",
    "            self.w=self.w-learning_rate*dw\n",
    "            self.b=self.b-learning_rate*db\n",
    "            \n",
    "            #计算loss \n",
    "            loss=self.get_loss(y,y_pre)\n",
    "            print(\"epoch:{}  loss:{}\".format(i,loss))\n",
    "                  \n",
    "    def predict(self,x):\n",
    "        y_pre=np.dot(x,self.w)+self.b\n",
    "        return y_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e64bb199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0  loss:45.67460433530197\n",
      "epoch:1  loss:40.03791719255146\n",
      "epoch:2  loss:35.17687837651508\n",
      "epoch:3  loss:30.98447268872405\n",
      "epoch:4  loss:27.36845077232203\n",
      "epoch:5  loss:24.24929173281639\n",
      "epoch:6  loss:21.558446874838527\n",
      "epoch:7  loss:19.23682576674433\n",
      "epoch:8  loss:17.233491196847\n",
      "epoch:9  loss:15.504534198576893\n",
      "epoch:10  loss:14.012104298797324\n",
      "epoch:11  loss:12.723573571705577\n",
      "epoch:12  loss:11.610816035928222\n",
      "epoch:13  loss:10.649586479846153\n",
      "epoch:14  loss:9.818984996119305\n",
      "epoch:15  loss:9.100995399322084\n",
      "epoch:16  loss:8.480087332355195\n",
      "epoch:17  loss:7.9428732739058265\n",
      "epoch:18  loss:7.477812871752134\n",
      "epoch:19  loss:7.074958071928668\n",
      "epoch:20  loss:6.7257334147715095\n",
      "epoch:21  loss:6.422746645544296\n",
      "epoch:22  loss:6.159625456862756\n",
      "epoch:23  loss:5.930876757272471\n",
      "epoch:24  loss:5.73176535783868\n",
      "epoch:25  loss:5.558209397465903\n",
      "epoch:26  loss:5.406690197350396\n",
      "epoch:27  loss:5.27417455364488\n",
      "epoch:28  loss:5.158047752120683\n",
      "epoch:29  loss:5.056055825414358\n",
      "epoch:30  loss:4.966255777574174\n",
      "epoch:31  loss:4.886972676584612\n",
      "epoch:32  loss:4.816762667230575\n",
      "epoch:33  loss:4.754381087417439\n",
      "epoch:34  loss:4.698754983776066\n",
      "epoch:35  loss:4.648959419542846\n",
      "epoch:36  loss:4.604197051459608\n",
      "epoch:37  loss:4.563780524636562\n",
      "epoch:38  loss:4.52711729655794\n",
      "epoch:39  loss:4.493696555059086\n",
      "epoch:40  loss:4.463077941350459\n",
      "epoch:41  loss:4.434881829029571\n",
      "epoch:42  loss:4.408780944386844\n",
      "epoch:43  loss:4.384493142934709\n",
      "epoch:44  loss:4.361775182625146\n",
      "epoch:45  loss:4.340417356233388\n",
      "epoch:46  loss:4.320238864360678\n",
      "epoch:47  loss:4.301083826866076\n",
      "epoch:48  loss:4.282817844637347\n",
      "epoch:49  loss:4.265325035765574\n",
      "epoch:50  loss:4.248505480665653\n",
      "epoch:51  loss:4.23227301971662\n",
      "epoch:52  loss:4.21655335478142\n",
      "epoch:53  loss:4.201282412677038\n",
      "epoch:54  loss:4.186404934451318\n",
      "epoch:55  loss:4.171873259309817\n",
      "epoch:56  loss:4.157646276335061\n",
      "epoch:57  loss:4.1436885208462995\n",
      "epoch:58  loss:4.129969395442421\n",
      "epoch:59  loss:4.116462498524309\n",
      "epoch:60  loss:4.1031450454667375\n",
      "epoch:61  loss:4.089997369656074\n",
      "epoch:62  loss:4.077002492373988\n",
      "epoch:63  loss:4.064145752027835\n",
      "epoch:64  loss:4.051414484539118\n",
      "epoch:65  loss:4.038797747831271\n",
      "epoch:66  loss:4.0262860843319865\n",
      "epoch:67  loss:4.013871316244846\n",
      "epoch:68  loss:4.001546369068805\n",
      "epoch:69  loss:3.9893051194679\n",
      "epoch:70  loss:3.977142264131345\n",
      "epoch:71  loss:3.965053206727809\n",
      "epoch:72  loss:3.9530339604572293\n",
      "epoch:73  loss:3.9410810640480323\n",
      "epoch:74  loss:3.929191509344589\n",
      "epoch:75  loss:3.9173626788856906\n",
      "epoch:76  loss:3.905592292095471\n",
      "epoch:77  loss:3.8938783588984767\n",
      "epoch:78  loss:3.882219139734475\n",
      "epoch:79  loss:3.8706131110899946\n",
      "epoch:80  loss:3.8590589357853844\n",
      "epoch:81  loss:3.8475554373612497\n",
      "epoch:82  loss:3.836101577998628\n",
      "epoch:83  loss:3.8246964394853187\n",
      "epoch:84  loss:3.813339206808081\n",
      "epoch:85  loss:3.802029154008358\n",
      "epoch:86  loss:3.7907656319892418\n",
      "epoch:87  loss:3.77954805800442\n",
      "epoch:88  loss:3.7683759065970523\n",
      "epoch:89  loss:3.757248701788496\n",
      "epoch:90  loss:3.7461660103444476\n",
      "epoch:91  loss:3.7351274359698317\n",
      "epoch:92  loss:3.7241326143042897\n",
      "epoch:93  loss:3.713181208607811\n",
      "epoch:94  loss:3.7022729060412747\n",
      "epoch:95  loss:3.69140741445982\n",
      "epoch:96  loss:3.680584459648301\n",
      "epoch:97  loss:3.6698037829378016\n",
      "epoch:98  loss:3.6590651391506595\n",
      "epoch:99  loss:3.6483682948286593\n",
      "epoch:100  loss:3.6377130267053297\n",
      "epoch:101  loss:3.6270991203886656\n",
      "epoch:102  loss:3.616526369225237\n",
      "epoch:103  loss:3.605994573320665\n",
      "epoch:104  loss:3.5955035386948904\n",
      "epoch:105  loss:3.5850530765536295\n",
      "epoch:106  loss:3.5746430026600042\n",
      "epoch:107  loss:3.5642731367925014\n",
      "epoch:108  loss:3.5539433022773785\n",
      "epoch:109  loss:3.5436533255852227\n",
      "epoch:110  loss:3.5334030359828317\n",
      "epoch:111  loss:3.5231922652327614\n",
      "epoch:112  loss:3.513020847333992\n",
      "epoch:113  loss:3.502888618298016\n",
      "epoch:114  loss:3.4927954159554857\n",
      "epoch:115  loss:3.4827410797891774\n",
      "epoch:116  loss:3.472725450789669\n",
      "epoch:117  loss:3.4627483713305844\n",
      "epoch:118  loss:3.4528096850607044\n",
      "epoch:119  loss:3.4429092368106295\n",
      "epoch:120  loss:3.4330468725119774\n",
      "epoch:121  loss:3.423222439127395\n",
      "epoch:122  loss:3.4134357845898835\n",
      "epoch:123  loss:3.403686757750167\n",
      "epoch:124  loss:3.39397520833099\n",
      "epoch:125  loss:3.3843009868873786\n",
      "epoch:126  loss:3.3746639447720628\n",
      "epoch:127  loss:3.3650639341053403\n",
      "epoch:128  loss:3.355500807748761\n",
      "epoch:129  loss:3.3459744192821246\n",
      "epoch:130  loss:3.3364846229833227\n",
      "epoch:131  loss:3.3270312738106385\n",
      "epoch:132  loss:3.3176142273871623\n",
      "epoch:133  loss:3.308233339987043\n",
      "epoch:134  loss:3.2988884685233106\n",
      "epoch:135  loss:3.289579470537061\n",
      "epoch:136  loss:3.2803062041878155\n",
      "epoch:137  loss:3.271068528244894\n",
      "epoch:138  loss:3.26186630207967\n",
      "epoch:139  loss:3.2526993856585653\n",
      "epoch:140  loss:3.243567639536717\n",
      "epoch:141  loss:3.234470924852193\n",
      "epoch:142  loss:3.225409103320705\n",
      "epoch:143  loss:3.216382037230739\n",
      "epoch:144  loss:3.2073895894390456\n",
      "epoch:145  loss:3.1984316233664587\n",
      "epoch:146  loss:3.1895080029939775\n",
      "epoch:147  loss:3.180618592859087\n",
      "epoch:148  loss:3.1717632580522817\n",
      "epoch:149  loss:3.1629418642137765\n",
      "epoch:150  loss:3.1541542775303695\n",
      "epoch:151  loss:3.145400364732426\n",
      "epoch:152  loss:3.136679993091004\n",
      "epoch:153  loss:3.12799303041506\n",
      "epoch:154  loss:3.1193393450487643\n",
      "epoch:155  loss:3.110718805868871\n",
      "epoch:156  loss:3.1021312822821834\n",
      "epoch:157  loss:3.093576644223061\n",
      "epoch:158  loss:3.085054762150985\n",
      "epoch:159  loss:3.076565507048177\n",
      "epoch:160  loss:3.0681087504172506\n",
      "epoch:161  loss:3.0596843642789064\n",
      "epoch:162  loss:3.051292221169659\n",
      "epoch:163  loss:3.0429321941395915\n",
      "epoch:164  loss:3.0346041567501434\n",
      "epoch:165  loss:3.0263079830719137\n",
      "epoch:166  loss:3.018043547682497\n",
      "epoch:167  loss:3.0098107256643267\n",
      "epoch:168  loss:3.0016093926025564\n",
      "epoch:169  loss:2.993439424582933\n",
      "epoch:170  loss:2.985300698189708\n",
      "epoch:171  loss:2.977193090503558\n",
      "epoch:172  loss:2.9691164790995077\n",
      "epoch:173  loss:2.9610707420448814\n",
      "epoch:174  loss:2.9530557578972623\n",
      "epoch:175  loss:2.9450714057024565\n",
      "epoch:176  loss:2.9371175649924792\n",
      "epoch:177  loss:2.9291941157835475\n",
      "epoch:178  loss:2.921300938574076\n",
      "epoch:179  loss:2.9134379143427016\n",
      "epoch:180  loss:2.9056049245462954\n",
      "epoch:181  loss:2.897801851118002\n",
      "epoch:182  loss:2.890028576465281\n",
      "epoch:183  loss:2.8822849834679563\n",
      "epoch:184  loss:2.8745709554762766\n",
      "epoch:185  loss:2.866886376308989\n",
      "epoch:186  loss:2.8592311302514077\n",
      "epoch:187  loss:2.8516051020535143\n",
      "epoch:188  loss:2.84400817692804\n",
      "epoch:189  loss:2.8364402405485745\n",
      "epoch:190  loss:2.828901179047677\n",
      "epoch:191  loss:2.821390879015001\n",
      "epoch:192  loss:2.8139092274954116\n",
      "epoch:193  loss:2.806456111987133\n",
      "epoch:194  loss:2.7990314204398845\n",
      "epoch:195  loss:2.7916350412530364\n",
      "epoch:196  loss:2.7842668632737673\n",
      "epoch:197  loss:2.7769267757952343\n",
      "epoch:198  loss:2.7696146685547496\n",
      "epoch:199  loss:2.762330431731955\n",
      "epoch:200  loss:2.7550739559470245\n",
      "epoch:201  loss:2.7478451322588535\n",
      "epoch:202  loss:2.7406438521632714\n",
      "epoch:203  loss:2.7334700075912446\n",
      "epoch:204  loss:2.7263234909071126\n",
      "epoch:205  loss:2.719204194906803\n",
      "epoch:206  loss:2.7121120128160743\n",
      "epoch:207  loss:2.705046838288755\n",
      "epoch:208  loss:2.6980085654049977\n",
      "epoch:209  loss:2.690997088669535\n",
      "epoch:210  loss:2.6840123030099443\n",
      "epoch:211  loss:2.67705410377492\n",
      "epoch:212  loss:2.6701223867325545\n",
      "epoch:213  loss:2.6632170480686224\n",
      "epoch:214  loss:2.656337984384876\n",
      "epoch:215  loss:2.6494850926973466\n",
      "epoch:216  loss:2.6426582704346493\n",
      "epoch:217  loss:2.6358574154363037\n",
      "epoch:218  loss:2.629082425951049\n",
      "epoch:219  loss:2.622333200635179\n",
      "epoch:220  loss:2.6156096385508736\n",
      "epoch:221  loss:2.608911639164545\n",
      "epoch:222  loss:2.6022391023451843\n",
      "epoch:223  loss:2.595591928362722\n",
      "epoch:224  loss:2.5889700178863873\n",
      "epoch:225  loss:2.5823732719830814\n",
      "epoch:226  loss:2.5758015921157535\n",
      "epoch:227  loss:2.569254880141784\n",
      "epoch:228  loss:2.562733038311375\n",
      "epoch:229  loss:2.5562359692659493\n",
      "epoch:230  loss:2.5497635760365522\n",
      "epoch:231  loss:2.5433157620422633\n",
      "epoch:232  loss:2.5368924310886114\n",
      "epoch:233  loss:2.530493487366002\n",
      "epoch:234  loss:2.524118835448144\n",
      "epoch:235  loss:2.5177683802904887\n",
      "epoch:236  loss:2.5114420272286706\n",
      "epoch:237  loss:2.5051396819769587\n",
      "epoch:238  loss:2.4988612506267147\n",
      "epoch:239  loss:2.4926066396448503\n",
      "epoch:240  loss:2.486375755872301\n",
      "epoch:241  loss:2.480168506522498\n",
      "epoch:242  loss:2.473984799179853\n",
      "epoch:243  loss:2.4678245417982447\n",
      "epoch:244  loss:2.46168764269951\n",
      "epoch:245  loss:2.455574010571949\n",
      "epoch:246  loss:2.4494835544688307\n",
      "epoch:247  loss:2.4434161838069026\n",
      "epoch:248  loss:2.4373718083649147\n",
      "epoch:249  loss:2.43135033828214\n",
      "epoch:250  loss:2.425351684056909\n",
      "epoch:251  loss:2.419375756545145\n",
      "epoch:252  loss:2.413422466958909\n",
      "epoch:253  loss:2.4074917268649503\n",
      "epoch:254  loss:2.401583448183257\n",
      "epoch:255  loss:2.395697543185624\n",
      "epoch:256  loss:2.389833924494216\n",
      "epoch:257  loss:2.3839925050801423\n",
      "epoch:258  loss:2.3781731982620378\n",
      "epoch:259  loss:2.3723759177046424\n",
      "epoch:260  loss:2.366600577417401\n",
      "epoch:261  loss:2.3608470917530533\n",
      "epoch:262  loss:2.3551153754062386\n",
      "epoch:263  loss:2.3494053434121054\n",
      "epoch:264  loss:2.343716911144925\n",
      "epoch:265  loss:2.33804999431671\n",
      "epoch:266  loss:2.332404508975845\n",
      "epoch:267  loss:2.326780371505709\n",
      "epoch:268  loss:2.321177498623324\n",
      "epoch:269  loss:2.315595807377988\n",
      "epoch:270  loss:2.3100352151499273\n",
      "epoch:271  loss:2.3044956396489535\n",
      "epoch:272  loss:2.298976998913117\n",
      "epoch:273  loss:2.2934792113073756\n",
      "epoch:274  loss:2.2880021955222656\n",
      "epoch:275  loss:2.2825458705725765\n",
      "epoch:276  loss:2.2771101557960334\n",
      "epoch:277  loss:2.2716949708519825\n",
      "epoch:278  loss:2.2663002357200877\n",
      "epoch:279  loss:2.2609258706990225\n",
      "epoch:280  loss:2.25557179640518\n",
      "epoch:281  loss:2.250237933771375\n",
      "epoch:282  loss:2.2449242040455637\n",
      "epoch:283  loss:2.239630528789558\n",
      "epoch:284  loss:2.2343568298777563\n",
      "epoch:285  loss:2.2291030294958687\n",
      "epoch:286  loss:2.2238690501396503\n",
      "epoch:287  loss:2.218654814613649\n",
      "epoch:288  loss:2.213460246029945\n",
      "epoch:289  loss:2.208285267806904\n",
      "epoch:290  loss:2.203129803667931\n",
      "epoch:291  loss:2.197993777640236\n",
      "epoch:292  loss:2.1928771140535956\n",
      "epoch:293  loss:2.187779737539126\n",
      "epoch:294  loss:2.182701573028061\n",
      "epoch:295  loss:2.1776425457505315\n",
      "epoch:296  loss:2.172602581234353\n",
      "epoch:297  loss:2.1675816053038157\n",
      "epoch:298  loss:2.162579544078483\n",
      "epoch:299  loss:2.1575963239719926\n",
      "epoch:300  loss:2.152631871690863\n",
      "epoch:301  loss:2.147686114233302\n",
      "epoch:302  loss:2.14275897888803\n",
      "epoch:303  loss:2.1378503932330926\n",
      "epoch:304  loss:2.132960285134694\n",
      "epoch:305  loss:2.1280885827460216\n",
      "epoch:306  loss:2.1232352145060873\n",
      "epoch:307  loss:2.118400109138568\n",
      "epoch:308  loss:2.1135831956506435\n",
      "epoch:309  loss:2.108784403331856\n",
      "epoch:310  loss:2.1040036617529587\n",
      "epoch:311  loss:2.099240900764779\n",
      "epoch:312  loss:2.094496050497083\n",
      "epoch:313  loss:2.089769041357438\n",
      "epoch:314  loss:2.085059804030097\n",
      "epoch:315  loss:2.080368269474867\n",
      "epoch:316  loss:2.0756943689259977\n",
      "epoch:317  loss:2.0710380338910683\n",
      "epoch:318  loss:2.066399196149878\n",
      "epoch:319  loss:2.0617777877533445\n",
      "epoch:320  loss:2.057173741022404\n",
      "epoch:321  loss:2.052586988546918\n",
      "epoch:322  loss:2.048017463184583\n",
      "epoch:323  loss:2.0434650980598437\n",
      "epoch:324  loss:2.0389298265628177\n",
      "epoch:325  loss:2.0344115823482145\n",
      "epoch:326  loss:2.0299102993342633\n",
      "epoch:327  loss:2.0254259117016495\n",
      "epoch:328  loss:2.020958353892447\n",
      "epoch:329  loss:2.016507560609066\n",
      "epoch:330  loss:2.0120734668131917\n",
      "epoch:331  loss:2.00765600772474\n",
      "epoch:332  loss:2.0032551188208076\n",
      "epoch:333  loss:1.9988707358346347\n",
      "epoch:334  loss:1.9945027947545655\n",
      "epoch:335  loss:1.9901512318230163\n",
      "epoch:336  loss:1.9858159835354476\n",
      "epoch:337  loss:1.9814969866393395\n",
      "epoch:338  loss:1.977194178133171\n",
      "epoch:339  loss:1.972907495265407\n",
      "epoch:340  loss:1.968636875533481\n",
      "epoch:341  loss:1.9643822566827964\n",
      "epoch:342  loss:1.9601435767057154\n",
      "epoch:343  loss:1.9559207738405626\n",
      "epoch:344  loss:1.9517137865706322\n",
      "epoch:345  loss:1.9475225536231944\n",
      "epoch:346  loss:1.9433470139685096\n",
      "epoch:347  loss:1.9391871068188453\n",
      "epoch:348  loss:1.9350427716274976\n",
      "epoch:349  loss:1.9309139480878172\n",
      "epoch:350  loss:1.926800576132239\n",
      "epoch:351  loss:1.9227025959313118\n",
      "epoch:352  loss:1.9186199478927422\n",
      "epoch:353  loss:1.9145525726604289\n",
      "epoch:354  loss:1.9105004111135135\n",
      "epoch:355  loss:1.906463404365427\n",
      "epoch:356  loss:1.9024414937629424\n",
      "epoch:357  loss:1.898434620885235\n",
      "epoch:358  loss:1.894442727542937\n",
      "epoch:359  loss:1.8904657557772102\n",
      "epoch:360  loss:1.8865036478588086\n",
      "epoch:361  loss:1.8825563462871517\n",
      "epoch:362  loss:1.878623793789406\n",
      "epoch:363  loss:1.874705933319555\n",
      "epoch:364  loss:1.8708027080574927\n",
      "epoch:365  loss:1.8669140614081081\n",
      "epoch:366  loss:1.8630399370003723\n",
      "epoch:367  loss:1.8591802786864406\n",
      "epoch:368  loss:1.8553350305407457\n",
      "epoch:369  loss:1.8515041368591034\n",
      "epoch:370  loss:1.8476875421578165\n",
      "epoch:371  loss:1.8438851911727872\n",
      "epoch:372  loss:1.8400970288586282\n",
      "epoch:373  loss:1.8363230003877817\n",
      "epoch:374  loss:1.8325630511496402\n",
      "epoch:375  loss:1.828817126749668\n",
      "epoch:376  loss:1.8250851730085336\n",
      "epoch:377  loss:1.821367135961239\n",
      "epoch:378  loss:1.817662961856254\n",
      "epoch:379  loss:1.8139725971546552\n",
      "epoch:380  loss:1.8102959885292729\n",
      "epoch:381  loss:1.806633082863827\n",
      "epoch:382  loss:1.8029838272520882\n",
      "epoch:383  loss:1.799348168997022\n",
      "epoch:384  loss:1.7957260556099477\n",
      "epoch:385  loss:1.7921174348096987\n",
      "epoch:386  loss:1.7885222545217863\n",
      "epoch:387  loss:1.7849404628775638\n",
      "epoch:388  loss:1.7813720082133988\n",
      "epoch:389  loss:1.7778168390698468\n",
      "epoch:390  loss:1.7742749041908255\n",
      "epoch:391  loss:1.7707461525227983\n",
      "epoch:392  loss:1.7672305332139553\n",
      "epoch:393  loss:1.7637279956134015\n",
      "epoch:394  loss:1.7602384892703469\n",
      "epoch:395  loss:1.756761963933301\n",
      "epoch:396  loss:1.7532983695492665\n",
      "epoch:397  loss:1.7498476562629444\n",
      "epoch:398  loss:1.7464097744159333\n",
      "epoch:399  loss:1.7429846745459383\n",
      "epoch:400  loss:1.7395723073859803\n",
      "epoch:401  loss:1.7361726238636104\n",
      "epoch:402  loss:1.732785575100122\n",
      "epoch:403  loss:1.7294111124097795\n",
      "epoch:404  loss:1.7260491872990311\n",
      "epoch:405  loss:1.7226997514657416\n",
      "epoch:406  loss:1.7193627567984178\n",
      "epoch:407  loss:1.7160381553754445\n",
      "epoch:408  loss:1.712725899464317\n",
      "epoch:409  loss:1.709425941520881\n",
      "epoch:410  loss:1.7061382341885745\n",
      "epoch:411  loss:1.7028627302976735\n",
      "epoch:412  loss:1.6995993828645388\n",
      "epoch:413  loss:1.696348145090867\n",
      "epoch:414  loss:1.6931089703629438\n",
      "epoch:415  loss:1.6898818122509058\n",
      "epoch:416  loss:1.6866666245079958\n",
      "epoch:417  loss:1.683463361069827\n",
      "epoch:418  loss:1.680271976053653\n",
      "epoch:419  loss:1.6770924237576312\n",
      "epoch:420  loss:1.6739246586600995\n",
      "epoch:421  loss:1.6707686354188516\n",
      "epoch:422  loss:1.6676243088704128\n",
      "epoch:423  loss:1.664491634029323\n",
      "epoch:424  loss:1.66137056608742\n",
      "epoch:425  loss:1.658261060413127\n",
      "epoch:426  loss:1.6551630725507434\n",
      "epoch:427  loss:1.652076558219737\n",
      "epoch:428  loss:1.6490014733140383\n",
      "epoch:429  loss:1.6459377739013425\n",
      "epoch:430  loss:1.6428854162224078\n",
      "epoch:431  loss:1.6398443566903638\n",
      "epoch:432  loss:1.6368145518900135\n",
      "epoch:433  loss:1.6337959585771478\n",
      "epoch:434  loss:1.630788533677857\n",
      "epoch:435  loss:1.6277922342878446\n",
      "epoch:436  loss:1.6248070176717528\n",
      "epoch:437  loss:1.6218328412624732\n",
      "epoch:438  loss:1.6188696626604788\n",
      "epoch:439  loss:1.6159174396331486\n",
      "epoch:440  loss:1.6129761301140981\n",
      "epoch:441  loss:1.6100456922025097\n",
      "epoch:442  loss:1.6071260841624697\n",
      "epoch:443  loss:1.6042172644223052\n",
      "epoch:444  loss:1.601319191573925\n",
      "epoch:445  loss:1.5984318243721618\n",
      "epoch:446  loss:1.5955551217341215\n",
      "epoch:447  loss:1.5926890427385263\n",
      "epoch:448  loss:1.589833546625072\n",
      "epoch:449  loss:1.586988592793776\n",
      "epoch:450  loss:1.5841541408043396\n",
      "epoch:451  loss:1.5813301503755022\n",
      "epoch:452  loss:1.5785165813844078\n",
      "epoch:453  loss:1.5757133938659635\n",
      "epoch:454  loss:1.5729205480122133\n",
      "epoch:455  loss:1.5701380041717028\n",
      "epoch:456  loss:1.5673657228488524\n",
      "epoch:457  loss:1.5646036647033343\n",
      "epoch:458  loss:1.5618517905494465\n",
      "epoch:459  loss:1.559110061355495\n",
      "epoch:460  loss:1.5563784382431753\n",
      "epoch:461  loss:1.5536568824869574\n",
      "epoch:462  loss:1.5509453555134725\n",
      "epoch:463  loss:1.5482438189009053\n",
      "epoch:464  loss:1.545552234378382\n",
      "epoch:465  loss:1.5428705638253717\n",
      "epoch:466  loss:1.5401987692710764\n",
      "epoch:467  loss:1.5375368128938367\n",
      "epoch:468  loss:1.53488465702053\n",
      "epoch:469  loss:1.532242264125979\n",
      "epoch:470  loss:1.5296095968323546\n",
      "epoch:471  loss:1.5269866179085883\n",
      "epoch:472  loss:1.5243732902697857\n",
      "epoch:473  loss:1.5217695769766353\n",
      "epoch:474  loss:1.5191754412348315\n",
      "epoch:475  loss:1.516590846394487\n",
      "epoch:476  loss:1.5140157559495635\n",
      "epoch:477  loss:1.5114501335372852\n",
      "epoch:478  loss:1.5088939429375745\n",
      "epoch:479  loss:1.5063471480724722\n",
      "epoch:480  loss:1.5038097130055752\n",
      "epoch:481  loss:1.5012816019414645\n",
      "epoch:482  loss:1.4987627792251446\n",
      "epoch:483  loss:1.4962532093414793\n",
      "epoch:484  loss:1.493752856914632\n",
      "epoch:485  loss:1.491261686707509\n",
      "epoch:486  loss:1.4887796636212027\n",
      "epoch:487  loss:1.48630675269444\n",
      "epoch:488  loss:1.48384291910303\n",
      "epoch:489  loss:1.4813881281593178\n",
      "epoch:490  loss:1.478942345311635\n",
      "epoch:491  loss:1.476505536143758\n",
      "epoch:492  loss:1.474077666374365\n",
      "epoch:493  loss:1.4716587018564982\n",
      "epoch:494  loss:1.4692486085770229\n",
      "epoch:495  loss:1.4668473526560948\n",
      "epoch:496  loss:1.4644549003466265\n",
      "epoch:497  loss:1.4620712180337554\n",
      "epoch:498  loss:1.4596962722343167\n",
      "epoch:499  loss:1.4573300295963165\n"
     ]
    }
   ],
   "source": [
    "#生成一个样本量为100 特征量为10的数据集\n",
    "sample=1\n",
    "feature=10\n",
    "test_x=np.random.rand(sample,10)\n",
    "#构造目标值 y=3+2*x1+5*x2-3*x3\n",
    "test_y=3+2*test_x[:,0]+5*test_x[:,1]-3*test_x[:,2]+np.random.randn(sample) # 增加一点噪音\n",
    "#np.random.randn 生成满足正态分布的随机数\n",
    "\n",
    "model=LR()\n",
    "model.fit(X,y)\n",
    "y_pre=model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd1d9bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "权重： [ 1.97688894  4.55458179 -2.43934566  0.63075076 -0.1380054  -0.56171908\n",
      " -0.34044909 -0.44139535 -0.42714006 -0.38436083]\n",
      "偏移： 3.8608794932122303\n"
     ]
    }
   ],
   "source": [
    "#构造数据集\n",
    "import numpy as np \n",
    "\n",
    "#生成一个样本量为100 特征量为10的数据集\n",
    "sample=100\n",
    "feature=10\n",
    "X=np.random.rand(sample,feature)\n",
    "#构造目标值 y=3+2*x1+5*x2-3*x3\n",
    "y=3+2*X[:,0]+5*X[:,1]-3*X[:,2]+np.random.randn(sample) # 增加一点噪音\n",
    "#np.random.randn 生成满足正态分布的随机数\n",
    "\n",
    "from sklearn.linear_model import LinearRegression # 线性回归模型\n",
    "from sklearn.metrics import mean_squared_error # 评价指标\n",
    "\n",
    "model=LinearRegression() \n",
    "model.fit(X,y)\n",
    "y_pre=model.predict(test_x)\n",
    "\n",
    "#显示权重系数\n",
    "weights = model.coef_\n",
    "intercept = model.intercept_\n",
    "\n",
    "print(\"权重：\", weights)\n",
    "print(\"偏移：\", intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "711829a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99330715]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# 示例\n",
    "x = np.array([5])\n",
    "print(sigmoid(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "736a1ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\narray([1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,\\n       1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\\n       0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,\\n       1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\\n       1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1])\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "#构造数据集\n",
    "#生成一个样本量为100 特征量为10的数据集\n",
    "sample=100\n",
    "feature=10\n",
    "X=np.random.rand(sample,10)\n",
    "#构造目标值 y=3+2*x1+5*x2-3*x3\n",
    "y=3+2*X[:,0]+5*X[:,1]-3*X[:,2]+np.random.randn(sample) # 增加一点噪音\n",
    "# 我们需要构建一个分类的问题\n",
    "mean_=np.mean(y)\n",
    "# 一半的数据低于平均值 一半的数据高于平均值\n",
    "y=np.where(y>=mean_,1,0)\n",
    "#样本就变成了分类事实，数据是否比平均值大\n",
    "\"\"\"\n",
    "array([1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
    "       1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
    "       0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
    "       1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
    "       1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1])\n",
    "\"\"\"\n",
    "\n",
    "#构造数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4e4213a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1, 100]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m y\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mwhere(y\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39mmean_,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#样本就变成了分类事实，数据是否比平均值大\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# 划分训练集和测试集\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# 创建逻辑回归模型\u001b[39;00m\n\u001b[0;32m     22\u001b[0m log_reg \u001b[38;5;241m=\u001b[39m LogisticRegression()\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2848\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2846\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2848\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2850\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   2851\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2852\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2853\u001b[0m )\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\sklearn\\utils\\validation.py:532\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m \n\u001b[0;32m    504\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;124;03m[[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>]\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    531\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 532\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\sklearn\\utils\\validation.py:475\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    473\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 475\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    476\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    478\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1, 100]"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np \n",
    "#构造数据集\n",
    "#生成一个样本量为100 特征量为10的数据集\n",
    "sample=100\n",
    "feature=10\n",
    "X=np.random.rand(sample,10)\n",
    "#构造目标值 y=3+2*x1+5*x2-3*x3\n",
    "y=3+2*X[:,0]+5*X[:,1]-3*X[:,2]+np.random.randn(sample) # 增加一点噪音\n",
    "# 我们需要构建一个分类的问题\n",
    "mean_=np.mean(y)\n",
    "# 一半的数据低于平均值 一半的数据高于平均值\n",
    "y=np.where(y>=mean_,1,0)\n",
    "#样本就变成了分类事实，数据是否比平均值大\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 创建逻辑回归模型\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# 训练模型\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# 预测\n",
    "predictions = log_reg.predict(X_test)\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee2f9c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
